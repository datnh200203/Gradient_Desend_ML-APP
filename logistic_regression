import numpy as np
import math

x = np.array([  0.245, 0.247, 0.285, 0.299, 0.327, 0.347, 0.356, 0.36, 0.363, 0.364, 
                0.398, 0.4, 0.409, 0.421, 0.432, 0.473, 0.509, 0.529, 0.561, 0.569, 
                0.594, 0.638, 0.656, 0.816, 0.853, 0.938, 1.036, 1.045])
y = np.array([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])

def logistic_function(z):
    return 1/(1 + np.exp(-z))

def predict(x, theta0, theta1):
    z = theta0 + theta1 * x
    gz = logistic_function(z)
    return gz

def cost_function(x, y_true, theta0, theta1):
    m = len(x)
    epsilon = 1e-15
    y_pred = predict(x, theta0, theta1)
    cost = (-1/m)*(np.sum(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon)))
    return cost

def gradient_descent(x, y, theta0, theta1, learning_rate):
    m = len(x)
    theta0_n = theta0 - learning_rate * (1/m) * np.sum(predict(x, theta0, theta1) - y)
    theta1_n = theta1 - learning_rate * (1/m) * np.sum((predict(x, theta0, theta1) - y) * x)
    theta0 = theta0_n
    theta1 = theta1_n
    return theta0, theta1

lap = 1000
learning_rate = 1e-9
np.random.seed()
theta0 = np.random.rand()
theta1 = np.random.rand()

for i in range(0, lap):
    theta0, theta1 = gradient_descent(x, y, theta0, theta1, learning_rate)

cost = cost_function(x, y, theta0, theta1)

print(theta0, theta1, cost)
